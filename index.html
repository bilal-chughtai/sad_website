<!DOCTYPE html>
<html lang="en-US">

<head>
    <meta charset="utf-8">
    <title>Situational Awareness Dataset</title>
    <link rel="apple-touch-icon" sizes="180x180" href="/assets/favicon/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="/assets/favicon/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="/assets/favicon/favicon-16x16.png">
    <link rel="manifest" href="/assets/favicon/site.webmanifest">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css">
    <link href="./custom.css" rel="stylesheet">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons@1.10.5/font/bootstrap-icons.css">
    <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
    <script>
        function loadContent() {
            var desktopContent = document.getElementById('desktop-content');
            var mobileContent = document.getElementById('mobile-content');
            if (window.innerWidth >= 1500) {
                if (!desktopContent.innerHTML) {
                    fetch('assets/plot.html')
                        .then(response => response.text())
                        .then(html => {
                            desktopContent.innerHTML = html;
                            // Re-run any scripts in the loaded content
                            Array.from(desktopContent.querySelectorAll('script')).forEach(oldScript => {
                                const newScript = document.createElement('script');
                                Array.from(oldScript.attributes).forEach(attr => newScript.setAttribute(attr.name, attr.value));
                                newScript.appendChild(document.createTextNode(oldScript.innerHTML));
                                oldScript.parentNode.replaceChild(newScript, oldScript);
                            });
                        })
                        .catch(error => console.error('Error loading plot:', error));
                }
                desktopContent.style.display = 'block';
                mobileContent.style.display = 'none';
            } else {
                desktopContent.style.display = 'none';
                mobileContent.style.display = 'block';
            }
        }
        window.addEventListener('load', loadContent);
        window.addEventListener('resize', loadContent);
    </script>
    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-WYC6C5SYSP"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag() { dataLayer.push(arguments); }
        gtag('js', new Date());

        gtag('config', 'G-WYC6C5SYSP');
    </script>
</head>



<body class="mt-5">
    <div id="header" class="container text-center">
        <h2>Me, Myself and AI:</h2>
        <h2>The Situational Awareness Dataset for LLMs</h2>
        <h6 class="mt-4 col-lg-8 mx-auto">
            <a class="link-info text-decoration-none" href="mailto:rudolf.laine@gmail.com">Rudolf Laine</a><sup>1</sup>,
            <a class="link-info text-decoration-none" href="#">Bilal Chughtai</a><sup>1</sup>,
            <a class="link-info text-decoration-none" href="#">Jan Betley</a><sup>2</sup>,
            <a class="link-info text-decoration-none" href="#">Kaivalya Hariharan</a><sup>3</sup>,
            <a class="link-info text-decoration-none" href="#">Jérémy Scheurer</a><sup>4</sup>,
            <a class="link-info text-decoration-none" href="#">Mikita Balesni</a><sup>4</sup>,
            <a class="link-info text-decoration-none" href="#">Marius Hobbhahn</a><sup>4</sup>,
            <a class="link-info text-decoration-none" href="#">Alexander Meinke</a><sup>4</sup>,
            <a class="link-info text-decoration-none" href="mailto:owaine@gmail.com">Owain Evans</a><sup>2</sup>
        </h6>
        <p class="mt-3"><sup>1</sup>Independent, <sup>2</sup>Constellation, <sup>3</sup>MIT, <sup>4</sup>Apollo
            Research
        </p>
    </div>


    <div id="icons" class="row justify-content-center mt-4 col-lg-8 mx-auto">
        <div class="col-lg-2 col-md-2 col-sm-2 d-flex flex-column align-items-center">
            <a class="link-info text-decoration-none text-center w-100" href="https://arxiv.org/abs/2407.04694">
                <div class="icon mb-2"><i class="bi-file-earmark-pdf" style="font-size: 4rem;"></i></div>
                <h5>Paper</h5>
            </a>
        </div>

        <div class="col-lg-2 col-md-2 col-sm-2 d-flex flex-column align-items-center">
            <a class="link-info text-decoration-none text-center w-100" href="https://github.com/LRudL/sad">
                <div class="icon mb-2"><i class="bi-github" style="font-size: 4rem;"></i></div>
                <h5>Code and Data</h5>
            </a>
        </div>

        <div class="col-lg-2 col-md-2 col-sm-2 d-flex flex-column align-items-center">
            <a class="link-info text-decoration-none text-center w-100" href="#results">
                <div class="icon mb-2"><i class="bi-bar-chart-line" style="font-size: 4rem;"></i></div>
                <h5>Results</h5>
            </a>
        </div>

        <div class="col-lg-2 col-md-2 col-sm-2 d-flex flex-column align-items-center">
            <a class="link-info text-decoration-none text-center w-100" href="/assets/data.csv" download>
                <div class="icon mb-2"><i class="bi-download" style="font-size: 4rem;"></i></div>
                <h5>Results CSV</h5>
            </a>
        </div>
    </div>

    <div id="content" class="container mt-5">
        <div class="row justify-content-center">
            <div class="text-center">
                <h3>Summary of Research</h3>
            </div>
        </div>

        <div id="summary_of_research" class="text-left col-lg-8 mx-auto">
            <p class="mt-4">
                AI assistants such as ChatGPT are trained to act like AIs, for example when they say "I am a large
                language model". However, do such models really know that they are LLMs and reliably act on this
                knowledge? Are they aware of their current circumstances, such as whether they are deployed? We refer to
                a model's knowledge of itself and its circumstances as <b>situational awareness</b>.
            </p>
            <p class="mt-4">
                The <b>Situational Awareness Dataset (SAD)</b> quantifies situational awareness in LLMs using a range of
                behavioral tests. The benchmark comprises 7 task categories, 16 tasks, and over 12,000 questions.
                Capabilities tested include the ability of LLMs to (i) recognize their own generated text, (ii) predict
                their own behavior, (iii) determine whether a prompt is from internal evaluation or real-world
                deployment, and (iv) follow instructions that depend on self-knowledge.
            </p>
            <p class="mt-4">
                While all models perform better than chance, even the highest-scoring model (Claude 3.5 Sonnet) is far
                from a human baseline on certain tasks. Performance on SAD is only partially predicted by MMLU score.
                Chat models, which are finetuned to serve as AI assistants, outperform their corresponding base models
                on SAD but not on general knowledge tasks.
            </p>
            <p class="mt-4">
                Situational awareness is important because it enhances a model's capacity for autonomous planning and
                action. While this has potential benefits for automation, it also introduces novel risks related to AI
                safety and control.
            </p>
        </div>

        <div id="sad_tasks" class="row justify-content-center">
            <div class="col-lg-10 content-wrapper">
                <div class="text-center mt-0">
                    <img src="assets/SAD.png" alt="Composition of SAD" class="img-fluid" ,
                        style="max-width: 900px; width: 100%;">
                </div>
                <figcaption class="mt-2 col-lg-10 mx-auto" style="font-size: 0.9em; font-style: italic;">Figure 1: An outline of
                    the structure of SAD. We
                    divide situational awareness into 3
                    aspects - self-knowledge, situational inferences, and taking actions. Within each aspect, we
                    have various categories of related tasks. There are 7 categories and 16 tasks in total. Examples
                    are shown on the right; see the paper appendices for more examples.</figcaption>
                </div>
            </div>
            
        <div id="plot" class="row justify-content-center">
            <h3 id="results" class="mt-5 text-center mx-auto">Latest Results</h3>
            <div class="col-lg-12">
                <div id="desktop-content"></div>
                <div id="mobile-content">
                    <img src="assets/static_plot.png" alt="Static plot image" style="width: 100%;">
                </div>
            </div>
        </div>
        <figcaption class="mt-2 col-lg-8 mx-auto"
            style="font-size: 0.9em; font-style: italic;">Figure 2: An
            interactive plot of the results of SAD on LLMs we test. SAD indicates the overall SAD score. SAD-lite and
            SAD-mini
            refer to subsets of SAD that some users may prefer to run; see the <a
                href="https://github.com/LRudL/sad/blob/main/README.md">README</a> for more details. Categories refer to
            the
            seven groupings of situational
            awareness being tested. The upper baseline is either based on a human baseline (where expert
            humans answered as if they were LLMs), or, where that does not make sense, it is 100% (we argue
            in the paper appendix that these tasks are clearly solvable).</figcaption>

    <div class="row justify-content-center mt-5">
        <div class="col-lg-8">
            <div class="mb-5">
                <img src="assets/results_table.png" alt="Results Table" class="img-fluid">
                <figcaption class="mt-4" style="font-size: 0.9em; font-style: italic;"> Table 1: A table of results
                    of SAD on LLMs we test. The first column is overall SAD score.
                    SAD-lite and SAD-mini refer to subsets of SAD that some users may prefer to run; see the <a
                        href="https://github.com/LRudL/sad/blob/main/README.md">README</a> for more details. The
                    following columns show scores for the 7 categories, grouped by which aspect of situational
                    awareness is being tested. The upper baseline is either based on a human baseline (where expert
                    humans answered as if they were LLMs), or, where that does not make sense, it is 100% (we argue
                    in the paper appendix that these tasks are clearly solvable). "Self-rec." is self-recognition,
                    "id-lev." is ID-leverage, "anti-im." is anti-imitation.</figcaption>
            </div>

            <div class="card bg-light mb-5">
                <div class="card-body">
                    <h5 class="card-title">Citation</h5>
                    <pre class="card-text font-monospace" style="white-space: pre-wrap; word-wrap: break-word;">
@misc{laine2024sad,
    title = {Me, Myself, and AI: The Situational Awareness Dataset (SAD) for LLMs},
    author = {Rudolf Laine and Bilal Chughtai and Jan Betley and Kaivalya Hariharan and
    Jeremy Scheurer and Mikita Balesni and Marius Hobbhahn and Alexander Meinke
    and Owain Evans},
    year = {2024},
    eprint = {2407.04694},
    archivePrefix = {arXiv},
    primaryClass = {cs.CL},
    url = {https://arxiv.org/abs/2407.04694}
}</pre>
                </div>
            </div>
        </div>
</body>




</body>

</html>
